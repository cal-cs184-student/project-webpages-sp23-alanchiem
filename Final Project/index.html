<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
    div.padded {
      padding-top: 0px;
      padding-right: 100px;
      padding-bottom: 0.25in;
      padding-left: 100px;
    }
  </style>
<title>Your Name  |  CS 184</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>
<body>
<br />
<h1 align="middle">Assignment 3-2: Additional Features to PathTracer</h1>
    <h2 align="middle">Nate Dyre, Alan Chiem</h2>
    <h2 align="middle">https://cal-cs184-student.github.io/project-webpages-sp23-alanchiem/proj3-2/index.html</h2>


<div align="center">
    <table style="width=100%">
        <tr>
            <td align="middle">
                <img src="images/titleimg.png" width="480px" />
                <figcaption align="middle"></figcaption>
        </tr>
    </table>
</div>

    <div class="padded">

        <h3 align="middle">Overview</h3>
        <p>
            For this project, we were able to achieve a few more features to our pathtracing implementation. The two features we chose
            to add were the ability to replicate the look of microfacet material and the ability to display a depth of field within an image.
            This project involved a lot of formulas and so it was very important for us to follow them carefully.
        </p>
<!--        * NOTE: For this project, you will choose TWO out of the four given parts to complete. One of those parts must be Part 1 or Part 2. In other words, you can choose any combination of two parts except the pair (Part 3, Part 4).-->


<!--        <h3 align="middle">Part 1. Mirror and Glass Materials</h3>-->

<!--        <p><b>-->
<!--            Show a sequence of six images of scene `CBspheres.dae` rendered with `max_ray_depth` set to 0, 1, 2, 3, 4, 5, and 100. The other settings should be at least 64 samples per pixel and 4 samples per light. Make sure to include all screenshots.-->
<!--        </b></p>-->
<!--        <p>-->
<!--            Your response goes here.-->
<!--        </p>-->
<!--        <br>-->
<!--        <p><b>-->
<!--            Point out the new multibounce effects that appear in each image. -->
<!--        </b></p>-->
<!--        <p>-->
<!--            Your response goes here.-->
<!--        </p>-->
<!--        <br>-->
<!--        <p><b>-->
<!--            Explain how these bounce numbers relate to the particular effects that appear. Make sure to include all screenshots.-->
<!--        </b></p>-->
<!--        <p>-->
<!--            Your response goes here.-->
<!--        </p>-->
<!--        <br>-->


        <h3 align="middle">Part 2. Microfacet Material</h3>
        <p><b>
<!--            Show a screenshot sequence of 4 images of scene `CBdragon_microfacet_au.dae` rendered with $\alpha$ set to 0.005, 0.05, 0.25 and 0.5. The other settings should be at least 128 samples per pixel and 1 samples per light. The number of bounces should be at least 5. Describe the differences between different images. Note that, to change the $\alpha$, just open the .dae file and search for `microfacet`.-->
            `CBdragon_microfacet_au.dae` rendered with alpha set to 0.005, 0.05, 0.25 and 0.5. (128 samples per pixel, 1 samples per light, number of bounces = 5)
        </b></p>
        <p>
            α represents the roughness of the macro surface. As we increase α, the surface appears more diffuse.
            To polish something means to smooth out a surface and usually results in a shinier object and so
            when we decrease α (polish the object), the surface appears glossier.
            <br><br>
            <u>Microfacet Material Implementation</u>
            <br><br>
            The distribution is the key characteristic of the surface in a microfacet BRDF model. We first implement the BRDF evaluation function
            according to <a href="https://cs184.eecs.berkeley.edu/sp23/lecture/14-23/material-modeling">this formula</a>. The numerator consists of
            three parts: the Fresnel term, the shadow-masking term, and the normal distribution function. The shadow-masking term was already implemented
            for us, so we had to implement the Fresnel term and the NDF according to the formulas below. An air-conductor (ex. metals) Fresnel term in particular
            is wavelength-dependent unlike air-dielectric (ex. glass) Fresnel terms. Because of this, we need to calculate the Fresnel terms for the R, G and B channels.



        </p>

        <div align="middle">
            <table style="width:70%">
                <tr align="center">
                    <td>
                        <img src="images/fresnel.png" align="middle" width="300px"/>
                        <figcaption>Air-conductor Fresnel term</figcaption>
                    </td>
                    <td>
                        <img src="images/ndf.png" align="middle" width="300px"/>
                        <figcaption>For the NDF, we use a Beckman distribution.</figcaption>
                    </td>
                </tr>
            </table>
        </div>

        <br>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/P2-dragon-a0.005.png" align="middle" width="400px"/>
                        <figcaption>α = 0.005</figcaption>
                    </td>
                    <td>
                        <img src="images/P2-dragon-a0.05.png" align="middle" width="400px"/>
                        <figcaption>α = 0.05</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/P2-dragon-a0.25.png" align="middle" width="400px"/>
                        <figcaption>α = 0.25</figcaption>
                    </td>
                    <td>
                        <img src="images/P2-dragon-a0.5.png" align="middle" width="400px"/>
                        <figcaption>α = 0.5</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br>
        <p><b>
<!--            Show two images of scene `CBbunny_microfacet_cu.dae` rendered using cosine hemisphere sampling (default) and your importance sampling. The sampling rate should be fixed at 64 samples per pixel and 1 samples per light. The number of bounces should be at least 5. Briefly discuss their difference.-->
            `CBbunny_microfacet_cu.dae` rendered using cosine hemisphere sampling and importance sampling. (64 samples per pixel, 1 samples per light, number of bounces = 5)

        </b></p>
        <p>
            We can see from the pictures below that using cosine hemisphere sampling results in a rougher image than using importance sampling. Since we are
            using the Beckman NDF to define how the microfacets' normals are distributed, we receive less noise by sampling according to the shape of the Beckman NDF.

            <br><br>
            <u>Importance Sampling Implementation</u>
            <br><br>
            Main Idea: Sample from two pdfs and combine to get the sampled microfacet normal and its pdf. Reflect the given w<sub>o</sub> according to sampled normal
            to get sampled light incident direction w<sub>i</sub>.

        <div align="middle">
            <table style="width:70%">
                <tr align="center">
                    <td>
                        <img src="images/pdfsforndf.png" align="middle" width="300px"/>
                        <figcaption>1. pdfs that resemble the Beckman NDF</figcaption>
                    </td>
                    <td>
                        <img src="images/samplepdfs.png" align="middle" width="300px"/>
                        <figcaption>2. sampling the pdfs</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/pdfofh.png" align="middle" width="300px"/>
                        <figcaption>3. calculate pdf of sampling h w.r.t. solid angle</figcaption>
                    </td>
                    <td>
                        <img src="images/pdfofwi.png" align="middle" width="300px"/>
                        <figcaption>4. calculate pdf of sampling w<sub>i</sub> w.r.t. solid angle</figcaption>
                    </td>
                </tr>
            </table>
        </div>


        </p>
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/P2-bunny-chs.png" align="middle" width="400px"/>
                        <figcaption>cosine hemisphere sampling</figcaption>
                    </td>
                    <td>
                        <img src="images/P2-bunny-is.png" align="middle" width="400px"/>
                        <figcaption>importance sampling</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br>
        <p><b>
<!--            Show at least one image with some other conductor material, replacing `eta` and `k`. Note that you should look up values for real data rather than modifying them arbitrarily. Tell us what kind of material your parameters correspond to. -->
            Various conductor materials. (128 samples per pixel, 1 samples per light, number of bounces = 5)
        </b></p>

        <p>
            We can change the material by replacing 'eta' and 'k'. Using <a href="https://refractiveindex.info/">this website</a>, we
            found values from real data to use as parameters. The website allows us to query any eta/k value according to various wavelengths.
            The fixed wavelengths used are: 614nm (red), 549nm (green), 466nm (blue).

        </p>

        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/P2-dragon-copper.png" align="middle" width="400px"/>
                        <figcaption>copper (eta: 0.33228 1.0162 1.2474, k: 3.1646 2.5785 2.4603)</figcaption>
                    </td>
                    <td>
                        <img src="images/P2-dragon-silver.png" align="middle" width="400px"/>
                        <figcaption>silver (eta: 0.059193 0.059881 0.047366, k: 4.1283 3.5892 2.8132)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/P2-dragon-a0.25.png" align="middle" width="400px"/>
                        <figcaption>gold (eta: 0.21646 0.42833 1.3284, k: 3.2390 2.4599 1.8661)</figcaption>
                    </td>
                    <td>
                        <img src="images/P2-dragon-platinum.png" align="middle" width="400px"/>
                        <figcaption>platinum (eta: 0.46138 0.46608 0.58870, k: 5.9022 5.0942 3.9742)</figcaption>
                    </td>
                </tr>
            </table>
        </div>


        <br>




<!--        <h3 align="middle">Part 3. Environment Lightl</h3>-->
<!--        <b>Pick one *.exr* file to use for all subparts here. Include a converted *.jpg* of it in your website so we know what map you are using.</b>-->
<!--        -->
<!--        <p><b>-->
<!--            In a few sentences, explain the ideas behind environment lighting (i.e. why we do it/how it works).-->
<!--        </b></p>-->
<!--        <p>-->
<!--            Your response goes here.-->
<!--        </p>-->
<!--        <br>-->
<!--        <p><b>-->
<!--            Show the *probability_debug.png* file for the *.exr* file you are using, generated using the `save_probability_debug()` helper function after initializing your probability distributions.-->
<!--        </b></p>-->
<!--        <p>-->
<!--            Your response goes here.-->
<!--        </p>-->
<!--        <br>-->
<!--        <p><b>-->
<!--            Use the `bunny_unlit.dae` scene and your environment map *.exr* file and render two pictures, one with uniform sampling and one with importance sampling. Use 4 samples per pixel and 64 samples per light in each. Compare noise levels. Make sure to include all screenshots.-->
<!--        </b></p>-->
<!--        <p>-->
<!--            Your response goes here.-->
<!--        </p>-->
<!--        <br>-->
<!--        <p><b>-->
<!--            Use a different image (if you did part 2, we recommend `bunny_microfacet_cu_unlit.dae`) and your environment map *.exr* file and render two pictures, one with uniform sampling and one with importance sampling. Use 4 samples per pixel and 64 samples per light in each. Compare noise levels. Make sure to include all screenshots.-->
<!--        </b></p>-->
<!--        <p>-->
<!--            Your response goes here.-->
<!--        </p>-->
<!--        <br>-->



        <h3 align="middle">Part 4. Depth of Field</h3>
<!--        <b>-->
<!--            For these subparts, we recommend using a microfacet BSDF scene to show off the cool out of focus effects you can get with depth of field!-->
<!--        </b>-->
        <p><b>
            Differences between a pinhole camera model and a thin-lens camera model.
        </b></p>
        <p>
            With a pinhole camera, everything is in focus. A thin-lens camera model allows us to imitate human eyes with finite apertures
            resulting in only some objects being focused. By putting a thin lens between the image plane and the plane of focus, we no longer
            shoot rays directly to a point in focus. Instead, we uniformly sample the lens to get a sample point and then calculate the direction of
            a ray from that sample point to the point in focus.

            <br><br>
            <u>Thin-lens Camera Model Implementation</u>
            <br><br>
        <div align="middle">
            <img src="images/thinlensmodel.png" align="middle" width="500px"/>
        </div>
            <br>
            We first convert to sensor in camera space and by using the new sensor coordinates we can create the red Ray.
            Then, we uniformly sample the disk representing the thin lens at pLens using rndR and rndTheta.
            <br>
            <img src="images/plens.png" align="middle" width="500px"/>
            <br>
            We then obtain pFocus by intersecting the plane of focus with the red Ray. The blue ray is then calculated as the ray
            that originates from pLens with a direction towards pFocus. Finally, we normalize the direction of the ray,
            convert back to world space, add pos to the ray's origin, and set the near and far clips accordingly.
            <br>





        </p>
        <br>
        <p><b>
            Various depths through a scene. (512 samples per pixel, 4 samples per light, number of bounces = 12)
        </b></p>
        <p>
            We can choose where to focus in an image by changing the focus distance. At a focus distance of 4.5, the first half of the dragon
            is clear while the background is blurry. By increasing the focus distance a little to 5, we instead focus on the second half the dragon.
            At a focus distance of 6, the back wall is in focus while the dragon is blurry. We can also observe how if we were to lower the focus by too much, the entire image is blurred.
        </p>

        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/P4-dragon-d2.5.png" align="middle" width="400px"/>
                        <figcaption>focus distance 2.5</figcaption>
                    </td>
                    <td>
                        <img src="images/P4-dragon-d4.5.png" align="middle" width="400px"/>
                        <figcaption>focus distance 4.5</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/P4-dragon-d5.png" align="middle" width="400px"/>
                        <figcaption>focus distance 5</figcaption>
                    </td>
                    <td>
                        <img src="images/P4-dragon-d6.png" align="middle" width="400px"/>
                        <figcaption>focus distance 6</figcaption>
                    </td>
                </tr>
            </table>
        </div>

        <br>
        <p><b>
            Various aperture sizes, all focused at the same point. (512 samples per pixel, 4 samples per light, number of bounces = 12)
        </b></p>
        <p>
            If we set our lens radius to 0, it will essentially be the same as using a pinhole camera. With a low lens radius, such
            as the first picture, everything still appears to be in focus. However, as we increase the lens radius everything except the
            point in focus becomes more blurry.
        </p>

        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/P4-dragon-b0.1.png" align="middle" width="400px"/>
                        <figcaption>lens radius 0.1</figcaption>
                    </td>
                    <td>
                        <img src="images/P4-dragon-d4.5.png" align="middle" width="400px"/>
                        <figcaption>lens radius 0.23</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/P4-dragon-b0.5.png" align="middle" width="400px"/>
                        <figcaption>lens radius 0.5</figcaption>
                    </td>
                    <td>
                        <img src="images/P4-dragon-b0.8.png" align="middle" width="400px"/>
                        <figcaption>lens radius 0.8</figcaption>
                    </td>
                </tr>
            </table>
        </div>

        <br>
        <h3 align="middle">Working with a Partner</h3>
        <p>
            Our way of collaboration once again involved one of us coding while the other serves as a viewer. The viewer is able to act as second
            perspective to catch small mistakes and provide additional insight on what/how we should implement something. This usually took place over
            zoom. We learned that this strategy is often more efficient/fun than each of us working alone.
        </p>

    </div>
</body>
</html>
